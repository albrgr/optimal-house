\documentclass[12pt,final,fleqn]{article}

% basic packages
\usepackage[margin=1in] { geometry }
\usepackage{amssymb,amsmath, bm}
\usepackage{verbatim}
\usepackage[latin1]{inputenc}
%\usepackage[OT1]{fontenc}
\usepackage{setspace}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage[font={bf}]{caption}
%\usepackage{pgfplots}
%\usepackage[font={bf}]{caption}
\usepackage{setspace}
\usepackage{latexsym}
%\usepackage{euscript}
\usepackage{graphicx}
\usepackage{marvosym}
%\usepackage[varg]{txfonts}  Older version of ``g'' in math.

% bibliography packages
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{}{,}
\bibliographystyle{apsr}
\renewcommand{\bibname}{References}

% hyperref options
\usepackage{color}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\newcommand*{\Appendixautorefname}{Appendix}
\renewcommand*{\sectionautorefname}{Section}
\renewcommand*{\subsectionautorefname}{Section}
\newcommand{\aref}[1]{\hyperref[#1]{Appendix~\ref{#1}}}

% packages for tables
\usepackage{longtable}
\usepackage{booktabs, threeparttable}
\usepackage{threeparttablex}
%\usepackage{tabularx}
% dcolumn package
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\captionsetup{belowskip=10pt,aboveskip=-5pt}
\usepackage{multirow}
% rotating package
\usepackage[figuresright]{rotating}
\usepackage{pdflscape}
\usepackage{subcaption}

% packages for figures
\usepackage{grffile}
\usepackage{afterpage}
\usepackage{float}
\usepackage[section]{placeins}

% theorem package
\usepackage{theorem}
\theoremstyle{plain}
\theoremheaderfont{\scshape}
\newtheorem{theorem}{Theorem}
\newtheorem{algorithm}{Algorithm}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newcommand{\qed}{\hfill \ensuremath{\Box}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\argmin}{arg\min}
\DeclareMathOperator{\argmax}{arg\max}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\providecommand{\norm}[1]{\lVert#1\rVert}
\renewcommand\r{\right}
\renewcommand\l{\left}
\newcommand\E{\mathbb{E}}
\newcommand\dist{\buildrel\rm d\over\sim}
\newcommand\iid{\stackrel{\rm i.i.d.}{\sim}}
\newcommand\ind{\stackrel{\rm indep.}{\sim}}
\newcommand\cov{{\rm Cov}}
\newcommand\var{{\rm Var}}
\newcommand\SD{{\rm SD}}
\newcommand\bone{\mathbf{1}}
\newcommand\bzero{\mathbf{0}}

% dotted lines in tables
%\usepackage{arydshln}

\usepackage{pdflscape}

% spacing between sections and subsections
\usepackage[compact]{titlesec}

% times new roman
%\usepackage{times}

% appendix settings
\usepackage[toc,page,header]{appendix}
\renewcommand{\appendixpagename}{\centering Appendices}
\usepackage{chngcntr}
\usepackage{etoolbox}
\usepackage{lipsum}


% file paths and definitions
\input{../output/ch1txtstats.txt}
\makeatletter
\newcommand*\ExpandableInput[1]{\@@input#1 }
\makeatother

\setlength{\mathindent}{1cm}
\allowdisplaybreaks[4]
\doublespacing
%\special{pdf: pagesize width 8.5truein height 11.0truein}

\titleformat{\subsection}
  {\itshape\Large}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\itshape\large}{\thesubsubsection}{1em}{}  
  

\begin{document}
\author{Devin Incerti}
\title{\textbf{The Optimal Allocation of Campaign Funds in House Elections}}
\date{\today}
\maketitle

\thispagestyle{empty}
\setcounter{page}{0}

\begin{singlespacing}
\begin{abstract}
How should the Democratic and Republican parties allocate resources in House elections? Are actual spending strategies optimal? This paper answers these question by using Bayesian election forecasts to estimate Stromberg's (2008) probabilistic voting model. The model provides real-time estimates of the marginal value of additional resources in a district during a campaign and can be used to compare actual spending patterns to the amount that should have been spent according to the model. The correlation between observed and optimal spending is over $0.5$ in each non-redistricting year from 2000 to 2010. These correlations are consistent across different types of campaign donors including the Democratic Congressional Campaign Committee and the National Republican Congressional Committee; various political action committees; and individuals. There is also evidence that spending patterns are based on maximizing total seats rather than the probability of winning a majority of seats.
\end{abstract}
\end{singlespacing}

\clearpage
\doublespacing

\section{Introduction}
How should political parties allocate resources across electoral districts? Are actual strategies optimal? This paper answers these questions with a model that generates real-time estimates of the marginal value of additional campaign resources in each district. The model is applied to elections in the U.S. House of Representatives and the amount that should have been spent on each candidate is compared with actual spending. 

The allocation of campaign resources is viewed as a competition for seats between Democrats and Republicans. This competition is analyzed using the probabilistic voting model of \citet{stromberg2008electoral}, which can be explicitly solved and directly estimated. Like \citet{snyder1989election}, I consider two different assumptions about goals: first, parties maximize the expected number of seats won, and second, parties maximize the probability of winning a majority of seats. In equilibrium, the parties spend the most on the closest elections under the first assumption, whereas under the second assumption, an additional factor---the probability that a seat is pivotal---impacts the marginal value of a dollar in a district.

The marginal value of additional spending depends on predicted vote shares as well as uncertainty at the district and national levels. I use two types of forecasting models to estimate the model. The first is a Bayesian hierarchical model that uses information about districts and candidates to provide a forecast as of September 1st of each election year. The second is a state-space model that uses the hierarchical model as a prior and incorporates all available district and national polls. Unlike the first model, the latter is capable of providing real-time forecasts at any date during a campaign. The hierarchical model is used to forecast each non-redistricting year election from 2000 to 2010 and the state-space model is used to analyze the 2010 election at various stages of the campaign.

These forecasts are used to identify districts that should have received the most contributions. The correlation between the amount that should have been spent according to the model and actual spending is over $0.5$ in each non-redistricting election year from 2000 to 2010. Surprisingly, these correlations are consistent across different types of campaign donors: the spending patterns of party committees, such as the Democratic Congressional Campaign Committee (DCCC) and the National Republican Congressional Committee (NRCC), whose strategies should be the most likely to be coordinated and concerned with electoral success, are \emph{no} more consistent with the model's predictions than the spending patterns of political action committees (PACs) or individuals. There is also no evidence that spending for Republican candidates differs from spending for Democratic candidates. Finally, as one might expect, these correlations become even stronger when incorporating polling data using the state-space model: the correlation reaches a peak of over 0.8 when comparing spending in the final month of the 2010 campaign with predicted spending based on a forecast made using all information up until that date.

Between 2000 and 2010, only one election---the 2008 election---was lopsided enough to yield spending strategies that were significantly sensitive to party goals. During that election, actual spending is highly correlated ($r \approx \cormaxseats2008$) with a seat maximization based spending strategy but not ($r \approx \cormaj2008$) one based on maximizing the probability of winning a majority of seats.\footnote{$r$ refers to Pearson's correlation coefficient.} This provides evidence that parties are more concerned with whether an election is close than whether it is pivotal. 

Overall, the Democratic and Republican parties appear to pursue campaign strategies that aim to maximize expected seat share. However, this does not mean that other factors do not help explain observed differences in spending across districts. For example, incumbents and candidates running in open seats raise more funds than challengers running against incumbents. Similarly, party leaders raise more funds than their counterparts and members of the House Committee on Financial Services raise considerably more funds from financial firms than other incumbents not on the committee. That said, other explanatory factors are only close to as predictive as election-motivated giving in cases in which donors have the largest non-electoral incentives---like financial service firms buying access to candidates. 

\section{Related Literature} \label{sec: related literature}
This paper builds on a number of studies focusing on the strategic allocation of resources in campaigns. These studies date back to the work of Brams and Davis (\citeyear{brams1973resource, brams19743}), who look at how presidential campaigns should allocate campaign resources to maximize their expected electoral vote. Using a model in which each candidate has an equal probability of winning the popular vote in each state, they argue that presidential campaigns should allocate resources roughly in proportion to the 3/2 power of the number of electoral votes in that state, which means that larger states should receive a disproportionate share of campaign resources. This result was then challenged by \citet{colantoni1975campaign}, who conclude that a modified proportional rule that accounts for the closeness of an election fits the data better.

These studies assume that parties maximize the expected electoral vote rather than the probability of winning the election. \citet{brams1973resource} posited that the implications of this distinction would be relatively minor. \citet{aranson1974election} support this notion by showing that the two goals are equivalent if the game is symmetric---which implies that the expected number of seats won by each party must be that same. However, as discussed by \citet{snyder1989election}, this symmetry assumption is strong and not realistic in real world settings. He shows that when this is not the case, the two goals yield different equilibria.

More recently, \citet{stromberg2008electoral} has developed a more general model that can be applied to actual campaigns. He assumes that presidential candidates maximize the probability of winning a majority of electoral college votes and then calculates the number of times that presidential candidates should have visited each state. He finds a very strong  correlation ($ \approx 0.9$) between predicted and observed visits. 

My paper differs from \citet*{stromberg2008electoral} in two important respects. First, I analyze U.S. House elections, so party goals are more ambiguous because it is not clear whether they should maximize expected seat share or the probability of winning a majority of seats.  My finding that parties maximize the expected number of seats and not the probability of winning a majority of seats is consistent with Jacobson and Kernell's (\citeyear{jacobson1985party}) assertion that every congressional seat is valuable so parties should aim to maximize seats. Second, I show how poll-based forecasts can be used go generate real-time estimates of the marginal value of campaign resources.

The foundations of Stromberg's model date back to the probabilistic voting models of \citet{lindbeck1987balanced} and \citet{dixit1996determinants} used to analyze electoral competition. In these models, two competing political candidates must determine which interest groups should receive favors in order to maximize their probability of winning an election. When the political candidates do not differ in the efficiency in which they make transfers to various group, these models yield a ``swing voter'' equilibrium---similar to ''close election equilibrium'' in this paper---where interest groups that are the most politically central receive the most favors.

The results here also shed some light on current debates regarding the motivations of campaign donors. These motivations are important because, as \citet{stratmann2005some} notes, the predicted determinants of campaign contributions depend on the assumptions regarding contributor goals. Four objectives commonly cited in the literature are that contributions are a consumption good, an investment in policy, a means to buy access to a politician, or a way to influence an election \citep[e.g.][]{ansolabehere2003there, stratmann2005some}. 

This paper's finding that parties should (and do) contribute the most to districts that have the largest probability of being close is consistent with one of the more robust findings in the literature---that contributors spend more on close elections \citep{kau1982general, jacobson1985money, poole1985patterns, stratmann1991campaign}.\footnote{These studies have two main problems that this paper avoids. First, the closeness of an election is typically measured with an ex-post measure of the electoral margin or the lagged vote from the previous election. This does not mimic the decision of contributors who must make choices prior to election day and have considerably more information available to them then the vote in the previous election. Second, since they are not driven by theory, they do not provide any guidance the functional form of the relationship between the closeness of an election and spending, which should depend on the uncertainty (and probability distribution) of the predicted vote.} In addition, the result that the financial industry donates more to members of the House Committee on Financial Services is consistent with research showing that candidates serving on congressional committees raise more money \citep{grier1991committee, romer1994empirical, kroszner1998interest}.  

The optimal strategies in this paper depend on the assumption that additional contributions increase the probability that a candidate will win an election. While it seems difficult to believe that this would not be true, there is a large literature in political science and economics examining this question. Early research showed that campaign spending by challengers was very influential while spending by incumbents was not \citep{jacobson1978effects, jacobson1980money,jacobson1985money}. More recent studies that have better accounted for endogeneity biases have tended that campaign spending does impact the vote \citep{gerber1998estimating, green1988salvation,erikson2000equilibria}. 

\section{Campaign Spending Model}
This section describes a version of Stromberg's \citeyear{stromberg2008electoral} model suitable for analyzing U.S. House elections. House elections differ from presidential elections in two important respects. First, all districts are worth one seat while states in presidential elections are weighted according to their electoral votes. Second, parties maximize both the probability of winning a majority of seats and the expected number of seats. 

\subsection{Set Up}
The model considers electoral competition between two parties, labeled Republican $R$ and Democrat $D$.\footnote{Third party candidates are ignored.} During the campaign, each party must decide how to optimally allocate funds across the 435 Congressional districts. More formally, party $J = D, R$ must choose expenditures in district $i$, $e^J_i$, subject to the resource constraint, 
\begin{align} \label{eqn: budget constraint}
\sum_{i=1}^{435} e^J_i \leq E^J,
\end{align}
where $E^J$ is the amount of money party $J$ has to spend on candidates.

The share of votes received by party $D$ in district $i$ is assumed to depend on four primary factors: spending by the national parties, predetermined characteristics of the district, the national political climate, and unknown shocks. The effect of the choice variable, $e^J_i$, is assumed to be an increasing concave function, $u(e^J_i)$, so that the effect of spending decreases with the amount of spending. The predetermined district characteristics and the national climate are known before the spending decision is made and can be used to make a prediction, $V_i$, of party $D$'s vote share. Finally, there are two sources of uncertainty, a national error, $\delta$, and a district specific error, $\epsilon_i$. The national errors represent uncertain national swings that affect all districts equally and the district errors are unpredictable swings unique to each district. Both error terms are independently drawn from normal distributions,
\begin{align}
h(\delta) = N(\delta|0, \sigma^2_\delta)
\end{align}
and 
\begin{align}
g_i(\epsilon_i) = N(\epsilon_i|0, \sigma^2).
\end{align}
Letting $u(e^D_i) - u(e^R_i) = \Delta u_i$, party $R$ will consequently win a district if,
\begin{align} \label{eqn: Republican wins}
\Delta u_i + V_i + \delta + \epsilon_i \leq 1/2.
\end{align}
The probability of a victory by party $R$ conditional on expenditures, $e^D_i$ and $e^R_i$, and the national swing, $\delta$, is therefore,
\begin{align*}
G_i(1/2 - \Delta u_i - V_i - \delta),
\end{align*}
where $G_i(\cdot)$ is the cumulative distribution function (CDF) of $\epsilon_i$. It follows that if $s_i$ is an indicator variable equal to 1 if party $R$ wins a district and $0$ if party $D$ wins, then $s_i = 1$ with probability $G_i(\cdot)$ and $s_i = 0$ with probability $1 - G_i(\cdot)$. The total number of Republican seats is $S = \sum_{i=1}^{435} s_i$. Furthermore, since the $G_i(\cdot)$ are independently (but not identically) distributed conditional on $\delta$, $S$ follows a Poisson binomial distribution with mean
\begin{align} \label{eqn: conditional seat mean}
\mu_S = \mu_S(\Delta u, \delta) =  \sum_{i=1}^{435} G_i(\cdot),
\end{align}
and variance,
\begin{align} \label{eqn: conditional seat variance}
\sigma_S = \sigma_S(\Delta u, \delta) = \sum_{i=1}^{435} G_i(\cdot)(1 - G_i(\cdot)),
\end{align}
where $\Delta \mu = \left(\Delta \mu_1, \Delta \mu_2, \ldots, \Delta \mu_{435} \right)$ represents the utility differences resulting from any allocation of campaign resources across districts by the two parties.

\subsection{Party Goals}
Optimal strategies depend on the objective of the national parties. Unlike in presidential campaigns where the goal is clearly to win the election, the goals of the parties in House campaigns are less straightforward. As a result, I consider two plausible objective functions. 

The first objective function assumes that parties simply maximize the expected number of House seats. For party $R$, this is just the expectation of $\mu_S$ over the national shocks,
\begin{align} \label{eqn: expected number of seats}
\E\left[S (\Delta \mu)\right] = \int \sum_{i=1}^{435} G_i(1/2 - \Delta u_i - V_i - \delta)h(\delta) d\delta.
\end{align}

A second possibility is that parties maximize the probability of winning a majority of seats. For party $R$ this is,
\begin{align}
P^R(\Delta \mu) = \int \rm{Pr}\left(\sum_{i=1}^{435} s_i > 218 \right) h(\delta)d \delta.
\end{align}

This function is more difficult to maximize than equation~\ref{eqn: expected number of seats} because it is not additively separable across districts; that is, party $R$'s optimal strategy in one district depends on its strategy in all other districts. In order to solve the problem analytically, \citet{stromberg2008electoral} suggests calculating the approximate probability of winning instead. Since the national and district shocks are independent, this can be done by using the Lyapunov Central Limit Theorem, which does not require random variables to be identically distributed. Using this approximation, the number of seats won by party $R$, $S=\sum_{i=1}^{435} s_i$, is asymptotically normally distributed with mean $\mu_S$ and variance $\sigma_S$. The approximate probability of party $R$ winning the election is then,
\begin{align}
P^R(\Delta \mu) = \int 1 - \Phi\left(\frac{218 - \mu_S(\Delta u, \delta)}{\sigma_S(\Delta u, \delta)}\right) h(\delta)d\delta,
\end{align}
where $\Phi(\cdot)$ is the standard normal cumulative density function.

\subsection{Equilibrium} \label{sec: equilibrium}
Let $e^j = (e_1^j, e_2^j, \ldots, e_{435}^j)$ be the allocation of campaign spending across districts by party $j$ and $f^R(e^D, e^R)$ be the objective function used by party $R$. A Nash equilibrium in pure strategies $(e^{D*}, e^{R*})$ is then characterized by,
\begin{align}
f^R(e^D, e^{R*}) \geq f^R(e^{D*}, e^{R*}) \geq f^R(e^{D*}, e^{R}),
\end{align}
where $e^D$ and $e^R$ satisfy the budget constraint in equation~\ref{eqn: budget constraint}. The game has a unique interior NE which satisfies,
\begin{align}
\frac{\partial f^J}{\partial e_i^J}= Q_i u'(e^{J*}_i)=\lambda^J,
\end{align}
where $Q_i = \partial f^J/ \partial \Delta u_i$ and $\lambda^J$ is the Lagrange multiplier for party $j$.\footnote{For a proof of uniqueness see \citet{stromberg2008electoral}.} Since $u'(e^J)$ is decreasing in $e^J$, the parties allocate more resources to districts with higher values of $Q_i$. The value of $Q_i$ depends on the choice of the objective function.

When parties maximize the expected number of seats, $Q_i$ is easy to calculate and is just, 
\begin{align} \label{eqn: Q_i expected seats}
Q_i = Q_i^{seats} = \int  g_i(1/2 - \Delta u_i - V_i - \delta) h(\delta) d\delta.
\end{align}
Not unexpectedly, districts with a predicted vote share close to 1/2 should receive the most expenditures. Spending should also be more concentrated when the error terms, $\sigma$ and $\delta$, are smaller. If on the other hand, parties maximize the probability of winning a majority of seats, $Q_i$ is more intricate. For party $R$, it is,
\begin{align} \label{eqn: Q_i probability of winning}
Q_i  = Q_i^{maj} &= - \int \left(\frac{\partial \Phi(\cdot)}{\partial \mu_S}\frac{\partial \mu_S}{\partial \Delta u_i}+\frac{\partial \Phi(\cdot)}{\partial \sigma_S}\frac{\partial \sigma_S}{\partial \Delta u_i}\right)h(\delta)d\delta  \\
& = \int \frac{1}{\sigma_S}\phi(x(\delta))g_i(\cdot)h(\delta)d\delta \nonumber \\
 &\qquad + \int \frac{1}{\sigma_S}\phi(x(\delta))x(k, \delta)\left(1-2G_i(\cdot)\right) g_i(\cdot)h(\delta)d\delta,
\end{align}
where $x(\delta) = \left(218 - \mu_S\right)/\sigma_S$. The first term is the effect of an increase in spending on the mean number of Republican seats while the second term is its effect on the variance. Parties have an incentive to influence the variance because they want to increase the probability of a desirable outcome. The trailing party will want to increase the variance to increase the probability of a major change in the election outcome while the leading party will want to do just the opposite. The trailing party can increase the variance by spending more on districts in which its candidate it losing and the leading party can decrease the variance by spending more on districts in which it is winning. Intuitively, the leading party does not need to worry about districts in which it is losing because it only needs to make sure that it holds on to the one's it is leading in.

As shown by \citet{stromberg2008electoral}, an alternative interpretation of equation~\ref{eqn: Q_i probability of winning} is that it is the probability that a district is 1) decisive in whether or not a party wins a majority of seats and 2) the district is a swing district. Following Stromberg, I call such a district a ``decisive swing district''. The probability of being a swing district is the probability that an electoral race is tied (or at least very close), while a district is decisive if winning (or losing) that district would make the difference between winning (or losing) a majority of seats. The idea that parties should spend more money on swing districts is consistent with the first order condition in equation~\ref{eqn: Q_i expected seats}. The idea that parties should spend more on decisive (also known as pivotal) districts differentiates the two maximization problems.

\subsection{Functional Form} \label{sec: functional form}
In order to solve for equilibrium spending, $e_i^{J*}$ and calculate $Q_i$, it is necessary to make an assumption about the functional form of $u(e_i^J)$. One functional form particularly amenable to empirical analysis is the logarithmic form, $u(e_i^J)= \theta \log (e_i^J)$, which results in the first order condition for district $k$ and party $J$,
\begin{align} \label{eqn: equilibrium spending}
e_k^{J*} = \frac{Q_k}{\sum Q_i}E^{J}.
\end{align} 
Each party spends the same fraction of the budget on district $k$, but $e_i^{R*}$ only equals $e_i^{D*}$ if both parties have identical budgets so that $E^{R} = E^{D}$.\footnote{For a formal proof that $e_i^{D*} = e_i^{R*}$ in equilibrium when $E^R = E^D$ see \citet{stromberg2008electoral}} Equation~\ref{eqn: equilibrium spending} implies that $Q_i$ is evaluated at $\Delta u_i = \theta \log(E^D/E^R)$, which reduces to $\Delta u_i=0$ when the budgets are equal. If $\theta$, $E^D$ or $E^R$ are unknown (and time-varying), then this term will be incorporated into the national shock, $\delta$, during estimation.

\section{Model Estimation}
To calculate $Q_i$, it is necessary to estimate the variances of the district and national shocks, $\sigma^2$ and $\sigma^2_\delta$, as well as the two-party vote, $V_i$. In the section I describe a Bayesian methodology that can estimate these parameters using historical political and economic information, and when available, polling data. The historical information provides a forecast of the election as of September 1st in each election year and polling data from September 1st through election day is used to update forecasts as campaigns progress. 

\subsection{A Bayesian Hierarchical Model}
\subsubsection{Modeling Approach}
Previous research shows that national elections are highly predictable from one year to the next using historical data \citep[e.g.][]{campbell1992forecasting, gelman1993american, kastellec2008predicting}. Consistent with these historical models and equation~\ref{eqn: Republican wins}, I model the Democratic share of the two party vote as a linear function of a matrix of explanatory variables, $X_{iy}$ (which are described in the online Appendix), national shocks, and district shocks,
\begin{align}
v_{iy}&= X_{iy}\beta + \delta_y + \epsilon_{iy}.
\end{align}
Since $\delta_y$ and $\epsilon_i$ are assumed to be normally distributed, this can be estimated using the Bayesian hierarchical model,
\begin{align}
v_{iy} &\sim N\left(X_{iy}\beta + \delta_y,  \sigma^2\right), \label{eqn: hierarchical linear model} \\
\delta_y &\sim N(0, \sigma^2_\delta), \label{eqn: hierarchical linear model time effects}
\end{align}
where $\beta$ is a vector of coefficients and $\delta_y$ is a random effect centered at 0. 

\subsubsection{Priors and Computation}
The hierarchical model was implemented using Stan, which uses Hamiltonian Monte Carlo sampling \citep{dev2016rstan}. Uniform hyperprior distributions were used for $\beta$, $\sigma$ and $\sigma_\delta$. The model was fit 5 separate times using post 1980 data to forecast (out-of-sample) the 2000, 2004, 2006, 2008 and 2010 elections.\footnote{For example, the 2000 election was fit using elections from 1980 to 1998; the 2004 election was fit using elections from 1980 to 2000 (recall that the model excludes years ending in 2); and so on.} Uncontested seats were dropped from the model because parties would have known which districts were uncontested by September 1st.\footnote{Signature-filing deadlines in districts are all before September 1st.}\footnote{I also dropped the few races with third party candidates. These do not impact the calculations of $Q_i^{maj}$ because they all occurred before 2000. Independents Bernie Sanders of Vermont and Virgil Goode of Virginia are classified as a Democrat and a Republican respectively. } The posterior distribution for each of the 5 models was simulated using the last half of 5 chains of $2,000$ iterations each. The Gelman and Rubin potential scale reduction factor $\hat{R}$ was approximately $1$ for all model parameters in all 5 fits, which suggests that the Markov Chain successfully converged each time.

\subsection{Incorporating Polls with a Bayesian DLM}
\subsubsection{Modeling Approach}
Although national elections are highly predictable using historical data, forecasts that incorporate polls become increasingly accurate as election day nears \citet{linzer2013dynamic}. A common method for combining polls with historical regressions is to treat polls as additional data points. This technique is commonly used by researchers and media outlets forecasting presidential and Senate elections.\footnote{See, for instance, the Huffington Post's forecasts at \url{http://elections.huffingtonpost.com/}, which are based on Simon Jackman's poll-tracking model; Drew Linzer's forecasts at \url{http://votamatic.org/}; neuroscientist Sam Wang's website \url{http://election.princeton.edu/}; and Nate Silver's \url{http://fivethirtyeight.com/}.} The primary difficulty is that the polls are highly correlated and should not be treated as independent data. As a result, forecasters attempt to average the polls so that the most informative ones receive the most weight. This weighted average of polls can then, in turn, be combined with information from the regression analysis, with weights that should be based on their respective variances.

As noted by Nate Silver (\citeyear{silver2014ratings}), the variability of a poll's forecast can be thought of as a function of three major components: sampling error, temporal error and pollster-induced error. The first two terms are relatively straightforward: sampling error occurs because each poll is based on a sample from the electorate and temporal error is due to uncertainty about opinion shifts between the date a poll is taken and election day. The final term, pollster-induced error, can be thought of as the error left over after accounting for sampling and temporal error. A major part of this residual term can be attributed to house effects, or time-invariant biases specific to certain pollsters. However, this error can also occur due to other polling difficulties such as undecided voters, respondents who will not vote in the actual election or respondents that do not express their true voting intentions.

Here, I utilize a state-space framework that can sequentially adjust forecasts as new polls become available. This model-based poll averaging approach is very similar to the the state-space poll-tracking model employed by Simon Jackman \citep{jackman2005pooling, jackman2009bayesian} and the forecasting model based on reverse-random walks used by Drew Linzer \citep{linzer2013dynamic}---which built on the idea used in \citet{strauss2007florida}. 

One important feature of House elections that must be accounted for is that district polling is sporadic both over time and across districts (see online Appendix). The incomplete nature of the data is important for calculating district and national errors because national errors based solely on district polling use less than half of all districts at any given date and the subset of districts available to calculate national errors changes over time (since different districts are polled at different times). One way around this is to to use the generic congressional vote as a measure of the national vote, since, unlike district polls, polling firms begin conducting polls of the generic congressional vote for the next election at a consistent rate almost as soon as election results are in.

To incorporate the national polls, I separate the vote in each district into the national vote and the district vote relative to the national vote as in \citet{lock2010bayesian} and \citet{{strauss2007florida}}. This separation allows me to use all available polling data to decompose national and local variation as required by the theoretical model.\footnote{Another strategy is to model the correlation between the national and district polls in a multivariate time-series model (see \citet{jackman2012prediction} for a brief explanation). This approach is not taken here because it is not consistent with the theoretical model used in this paper.}

The model of the national vote follows the model employed in \citet{jackman2005pooling} and \citet{jackman2009bayesian}, which provides a framework for ``pooling'' the polls over the course of a campaign. To set notation, let $t=1,\ldots,T$ represent days of the campaign where $t=1$ corresponds to the first day of the campaign season and $t=T$ is election day. Furthermore, let $k$ index a poll with sample size $N_k$ so that the number of respondents, $n_k$, from poll $k$ who support the Democratic candidate follows a binomial distribution,
\begin{align}
n_k \sim \rm{Bin}(N_k, \pi_k),
\end{align}
where $\pi_k$ is the proportion of voters from poll $k$ who intend (or report to intend) to vote for a (generic) Democratic candidate. Since the sample size of each poll is relatively large, the observed proportion of respondents who report that they intend to vote democratic, $y_k = n_k/N_k$ is approximated well by a normal distribution,
\begin{align}
y_k \approx N(\pi_k, \sigma^2_k),
\end{align}
where $\sigma^2_k= y_k(1-y_k)/N_k$. However, the parameter of underlying interest is not $\pi_k$, but the actual state of national opinion at time $t$. The $\pi_k$ are consequently modeled as a function of two components: the actual state of opinion, $\mu_t$, and a house effect, $\lambda_j$, specific to polling firm $j = 1,\ldots, J$,
\begin{align} \label{eqn pi}
\pi_k = \mu_{t[k]} + \lambda_{j[k]}.
\end{align}
Equation~\ref{eqn pi} is not identified because one could shift $\mu_{t[k]}$ up/down and $\lambda_{j[k]}$ down/up by the same constant without changing the value of $\pi_k$. As a result, I use the identifying restriction that the house effects sum to zero, $\sum_j \lambda_j = 0$.

As currently specified, the model only provides a snapshot of national opinion on any given day. To forecast the election, it is necessary to estimate $\mu_T$, which is an estimate for national opinion on the day of the election. Since forecasts are made on days $t' <T$, it is therefore necessary to make assumptions about the movement of $\mu_t$ from one day of the campaign to the next. Since there is no reason to expect there to be any trends in polling, it is reasonable to expect $\mu_t$ to follow a random walk, so that the full model can be written as,
\begin{align}
\label{observation eqn national}
y_k &= \mu_{t} + \lambda_{j} + v_k, \; v_k\sim N(0, \sigma^2_k)\\ 
\label{system eqn national}
\mu_{t} &= \mu_{t-1} + w_\mu, \; w_\mu \sim N(0, \sigma^2_{\mu}),
\end{align}
where $\sigma^2_\mu$ is an estimate of the daily change in $\mu_t$. As shown in~\autoref{sec: DLM Form}, equation~\ref{observation eqn national} and equation~\ref{system eqn national} form a state-space model, or more specifically, (since the model is linear and errors terms are Gaussian) a dynamic linear model (DLM). Equation~\ref{observation eqn national} is known as the observation equation while equation~\ref{system eqn national} is known as the state equation. 

A model for the district vote relative to the national vote proceeds in the same manner as the model for the national vote, but with a few additional differences. The first difference is that it is impractical to correct for house effects because there are only a few polls published per polling firm. The second difference is that national opinion at time $t$ is not actually observed, so the relative vote cannot be observed either. In practice, this is not a large problem because national opinion, $\mu_t$, is estimated very precisely using equation~\ref{observation eqn national} and equation~\ref{system eqn national} due to the abundance of large national polls.\footnote{The standard deviation of $\mu_t$ is typically around $0.004$.}

For the model of the relative district vote, let $l$ index district polls and continue to let $i$ index a district. Define the deviation of a district poll from the national vote as $d_l = y_l - \mu_{t[k]}$. The state-space model for the relative district vote is then,
\begin{align}
\label{observation eqn district}
d_l &= \xi_{it} + v_l, \; v_l\sim N(0, \sigma^2_l)\\ 
\label{system eqn district}
\xi_{it} &= \xi_{i,t-1} + w_\xi, \; w_\xi \sim N(0, \sigma^2_{\xi}), 
\end{align}
where $\sigma^2_l = y_l(1-y_l)/N_l$, $N_l$ is the sample size of the $l$th poll, $\xi_{i[l]t[l]}$ is an estimate of the deviation of opinion in state $i$ from national opinion at time $t$, and $\sigma^2_{\xi}$ captures the variance of day to day movements in $\xi$. Equation~\ref{observation eqn district} and equation~\ref{system eqn district} are just a simple multivariate extension of the model of the national vote that ignores house effects.

The overall forecast of the two-party vote for Democrats from each district is $\mu_T$ + $\xi_{iT}$. Separate forecasts of $\mu_T$ and $\xi_{iT}$ can basically be estimated using a Kalman filter\footnote{The Kalman filter is commonly used in engineering to track the movement of objects such as satellites or aircraft that are measured with noisy data. It is also frequently used in Macroeconomics and in political science to track public opinion.}, which the caveat that Bayesian MCMC techniques are needed to estimate the variance of the state equations and the house effects in the model for the national vote. The Kalman filter is instructive because it quantifies the relative weight attached to previous versions of the states and new polls. For example, using the Kalman filter, the mean and variance of the latent states in district $i$ on day $t$ given polls up to day $t$, $Y_{1:t}$ are,
\begin{align}
m_{it} &= E(\xi_{it}|Y_{1:t})= \left[\frac{m_{i,t-1}}{C_{i,t-1} + \sigma_\xi^2} + \sum_{l \in \mathcal{P}_{it}} \frac{d_{l}}{\sigma^2_l}\right]\cdot C_{it},\\
C_{it} &= \var(\xi_{it}|Y_{1:t})= \left[\frac{1}{C_{i,t-1} + \sigma_\xi^2}+ \sum_{l \in \mathcal{P}_{it}}\frac{1}{\sigma^2_{l}}\right]^{-1},
\end{align}
where $\mathcal{P}_{it}$ refers to the set of all polls published for district $i$ on day $t$.\footnote{There is almost never more than one poll for a given district and time period of a reasonably short duration (such as two weeks). For national elections, there are almost always multiple polling firms surveying on a given day or time period.}
The mean of $\xi_{it}$ is thus a weighted average of its mean on the previous day, $m_{i,t-1}$ and the deviation of all new polls from national opinion, $\sum_{l \in \mathcal{P}_{it}} d_{l}$, with weights proportional to their respective precisions. The precision of each poll is equal to the inverse of its sampling error and the precision of $m_{t-1}$ is the inverse of the sum of its variance and error in the movement of the states from one period to the next. The variance of $\xi_{it}$ is just the inverse of the poll precision plus the prior ($m_{t-1})$ precision. The interpretation for $\mu_t$ is identical except that new estimates are a weighted average of prior states and new polls less the democratic bias of the polling firm (i.e. $y_{k} - \delta_{j[k]}$).

To incorporate information from the historical regression, I treat forecasts from the hierarchical model as pseudo election day polls for both the national and district models. The two-party vote given to the national pseudo poll is the mean average district vote from the hierarchical model and the Democratic vote share given to each district's pseudo poll is the mean of the district forecast from the regression less the mean of average district vote. The corresponding variances are calculating using the posterior predictive distributions of these quantities.\footnote{See~\autoref{sec: Results from the DLM} for more details.} The regression forecasts in the national and district models consequently receive weights proportional to the regression forecast errors of the average district and relative district vote respectively. 

When estimating $\mu_T$ and $\xi_T$ prior to the election, there will be gaps between the last published poll and the pseudo poll from the regression. The Kalman filter helps bridge this gap by pushing the latent states forward toward election day. The relative weights received by the regression analysis and the polls depends largely on the number of days until the election. For instance, due to the random walk assumption, the precision of $\mu_{T-1}$ for a forecast of the national vote made on day $t'$ is equal to $1/\left[C_{i,t'} + (T-t')\cdot \sigma^2_\mu\right]$, which is linearly decreasing in time and in the day to day movements of the states. It follows that the regression analysis receives more weight when the election day is far away and when there is more movement in the polls.  

\subsubsection{Priors and Computation}
The Kalman filter assumes that the variances of the states are known. Since, in practice, this is clearly not the case, I estimate all of the model parameters jointly using Bayesian methods. To do so, I assign the unknown variance parameters, $\sigma_\mu^2$ and $\sigma_\xi^2$, inverse gamma priors. The house effects, $\lambda_j$, are given a normal prior centered at $0$. The posterior density is simulated with a Gibbs sampler, which is described in~\aref{Gibbs}.

To reduce the computational burden, campaign days were divided into two week periods. When a district (in the district model) or polling firm (in the national model) has multiple polls during a period I take an average of the polls weighted by sample size. This average poll is then given a sample size equal to the sum of the sample sizes of the individual polls. Aggregating the polls in this manner does not have a substantial impact on the forecasts for a number of reasons. Firstly, academic research typically shows that frequent polling provides very little information \citep[e.g.][]{gelman1993american, lock2010bayesian}. This is likely to be especially true in a window as short as two weeks. Secondly, there are very few cases in which a particular district was polled more than once in any two week span. 

To remain relatively agnostic about biases, I set the prior variance for the House effects $\sigma^2_\lambda$, equal to $0.05^2$. This allows for very biased polling firms because plus or minus 2 standard deviations from $0$ is plus or minus 10 percentage points. In addition, the inverse gamma priors for the unknown variance parameters, $\sigma_\mu^2$ and $\sigma_\xi^2$, were assigned shape parameters equal to $9$ and scale parameters equal to $0.02$, corresponding to a mean of $0.05^2$. The prior implies that period to period movements should be around 5 percentage points, but its effect is in practice dwarfed by the data. Election day polls were added to both the national and district models. The final poll in the national model is given a mean and standard deviation of $\finpollnatmean$ and  $\finpollnatsd$ respectively, which are the mean and standard deviation of the posterior predictive distribution of the average district vote from the hierarchical model.\footnote{Uncontested seats are assumed to be known with certainty. Uncontested Democratic (Republican) seats are given a vote share of 0.75 (0.25). These values are based on those reported in \citet{king1991systemic} and \citet{gelman1994unified} and derived from vote shares received by districts in the last year before they became uncontested and the first election after they became uncontested. For another paper that uses this strategy see \citet{kastellec2008predicting}.} Similarly, the mean and standard deviation of the election day poll for the district model are the mean and standard deviation of the posterior predictive distribution of the difference between the vote in each district and the average district vote. For reference, the mean of this standard deviation is $\finpollsd$, with is nearly identical to the mean of the district level error, $\sigma$.

The posterior density of both the national and district DLM's were simulated using $6,000$ iterations of the Gibbs Sampler with a burn-in of $1,000$ iterations. The model is (retrospectively) estimated every two weeks during the month's of September and October. Polling data after a desired forecasting date is removed to ensure that only data that would have been available at the time is used. This yields four separate forecasts of the 2010 election in addition to the prior from the hierarchical model: a mid September (1.5 months prior to election day), late-September (1 month prior to election day), mid-October (2 weeks prior to election day) and late-October (just before the election) forecast. Trace plots of the parameters suggest that the Markov Chain converged each time and are available upon request. (See the online Appendix for a estimates of the variance parameters).

\section{Forecast Performance and Calculations of $\mathbf{Q_i}$}
\subsection{Results from Bayesian Hierarchical Model} \label{sec: results hierarchical model}
In the Bayesian context, the forecast for an election in year $\tilde{y}$ is a probability distribution over $v_{i\tilde{y}}$, called a posterior predictive distribution, which uses data from election years $y < \tilde{y}$ to form a distribution for the unobserved (to the political analyst at the time) election in year $\tilde{y}$. Given the persistent nature of the data, it is not surprising that the model is able to forecast elections quite accurately. Indeed, if the forecast, $f_{iy}$, is the mean of the posterior predictive distribution of $v_{iy}$, then the root mean square forecast error (RMSFE), $\sqrt{E[(f_{iy} - v_{iy})^2]}$, is $\bhmrmse$ and the winner is only picked incorrectly $\bhmwrongwinner\%$ of the time. (See the online Appendix for a summary of the model fit.) 

There is also evidence that the decomposition of the error into a district component and a national component captures uncertainty in elections quite well. To see this, consider~\autoref{fig:Forecasts of the Number of the Percentage of Democratic Seats}, which is a histogram of the posterior predictive distributions of the fraction of seats won by Democrats in each of the five non-redistricting year elections from 2000 to 2010.\footnote{Uncontested seats are not included in these percentages.} The posterior predictive distribution for the number of seats is formed by simply counting the number of districts that had vote shares over $0.5$ for each of the $5,000$ draws from the posterior predictive distributions of $v_{i\tilde{y}}$ for each forecast year. The actual fraction of Democratic seats is usually within the $95\%$ credible interval, but the intervals are not unreasonably wide either. The 2010 election is a bit of an outlier since nearly all of the close elections broke toward the Republicans and large number of Democratic incumbents were defeated by Republican challengers. 

\begin{figure}[!htb]
\centering
\includegraphics{../figs/seats_hist.pdf}
\vspace{.5cm}
\caption{Forecasts of the Number of the Percentage of Democratic Seats}
\label{fig:Forecasts of the Number of the Percentage of Democratic Seats}
\begin{minipage}{\linewidth}
\footnotesize
\emph{Notes:} The histograms are from the posterior predictive distribution of the hierarchical model for each election year.
\end{minipage}
\end{figure}

\subsection{Results from the DLM} \label{sec: Results from the DLM}
Summaries of model's forecasts are are presented in figures~\ref{fig:Forecasts of the 2010 National Vote by Date} and \ref{fig:Forecasts of the Democratic Fraction of Seats in the 2010 Election}. \autoref{fig:Forecasts of the 2010 National Vote by Date} focuses on the forecast of the national vote. The dark dotted line is the DLM forecast (mean of the posterior distribution of $\mu_T$) and the error bars are $95\%$ credible intervals; from top to bottom, the light dotted lines are the regression based forecast from the hierarchical model, the actual average district vote and an average of polls from the latest available two-week period.\footnote{The actual average district vote includes uncontested seats, which were given vote shares of 0.25 in Republican districts and 0.75 in Democratic districts.} Forecasts are reported for the standard model that include the regression estimate as a final pseudo poll (the ``prior'' model) and an alternative specification that does not (the ``no prior'' model). Standard errors are considerably smaller for the ``prior'' forecast than the ``no prior'' forecast although the difference shrinks as election day gets closer because there is less and less temporal error in the polls. Due to this temporal error, the forecast of the ``prior'' model does not deviate from the regression forecast until mid-October, but as temporal error decreases more weight is placed on the polls and the forecast is eventually nearly identical to the actual average district vote. On the other hand, the ``no prior'', or polls only model, hugs the polling average closely and predicts that the national vote will be considerably more Republican. The ``no prior'' model lies slightly above the polling average though because the model accounts for House effects (see the online Appendix) and the most prolific pollsters (Rasmussen Reports and Gallup) leaned Republican during the campaign. 

\begin{figure}[!htb]
\centering
\includegraphics{../figs/dlmnatpred.pdf}
\vspace{.5cm}
\caption{Forecasts of the 2010 National Vote by Date}
\label{fig:Forecasts of the 2010 National Vote by Date}
\begin{minipage}{\linewidth}
\footnotesize
\emph{Notes:} The plot labeled ``No prior" plots results from the DLM that does not include a final national pseudo poll based on the regression analysis; the plot labeled ``Prior" includes this final pseudo poll. Vertical bars are $95\%$ credible intervals for the forecast of the national vote from the DLM (points are median of the simulated vote). The upper dotted line is the point estimate of the regression based forecast (the mean of posterior distribution of the average district vote from the hierarchical model). The middle dotted line is the actual average district vote for Democrats. The lower dotted line is an average of all national polls during each time period (i.e. the poll average at the 2 month mark is all polls from days between 2 and 1.5 months before the election).
\end{minipage}
\end{figure}

\autoref{fig:Forecasts of the Democratic Fraction of Seats in the 2010 Election} displays forecasts of the fraction of seats won by the Democrats after excluding uncontested seats. The upper light dotted line labeled ``regression forecast'' is identical to the mean of the histogram for the 2010 election in~\autoref{fig:Forecasts of the Number of the Percentage of Democratic Seats} and the reported fraction of actual seats won excludes uncontested seats as well. The forecast (and 95\% credible intervals) are calculated by taking the fraction of seats with a predicted vote share greater than $0.5$ for each simulated draw of $\mu_T + \xi_{iT}$. Like the forecast of the national vote, the ``prior'' model does not begin to move away from the regression forecast until mid-October and the ``no prior'' forecast changes considerably more from period to period. The ``no prior'' model provides a forecast of the Democratic seat share that is very close to the actual Democratic seat share while the ``prior'' model overestimates the electoral success of the Democrats. 

\begin{figure}[!htb]
\centering
\includegraphics{../figs/dlmseats.pdf}
\vspace{.5cm}
\caption{Forecasts of the Democratic Fraction of Seats in the 2010 Election}
\label{fig:Forecasts of the Democratic Fraction of Seats in the 2010 Election}
\begin{minipage}{\linewidth}
\footnotesize
\emph{Notes:} The plot labeled ``No prior" plots results from the DLM that does not include a final pseudo polls based on the regression analysis; the plot labeled ``Prior" includes these final pseudo polls. Vertical bars are $95\%$ credible intervals for the forecast of the Democratic fraction of seats from the DLM (points are median of the simulated vote). The upper dotted line labeled ``Regression forecast'' is the mean of the posterior distribution of the Democratic seat share from the hierarchical model. The lower dotted line is the fraction of seats won by Democrats. All calculations omit uncontested seats.
\end{minipage}
\end{figure}

It is tempting to use this evidence to discount the regression based prior but it is worth remembering that 2010 is an outlier election, nearly all of the close elections broke toward the Republicans, and comparing predicted seats to the observed seats is only one of many possible ways to evaluate forecasts (see the online Appendix for two alternative evaluation measures). 

\subsection{Calculating $Q_i$} \label{sec: calculating Q}
Armed with parameter estimates from the hierarchical model and the DLM, one can use the estimates of $V_{i}$, $\sigma$ and $\sigma_\delta$ to calculate the $Q_i$'s from equations~\ref{eqn: Q_i expected seats} and \ref{eqn: Q_i probability of winning}. In the theoretical model, these parameters are assumed to be known with certainty. But, when using a Bayesian approach this is not the case since the parameters have their own probability distributions. 

Accounting for this additional uncertainty with the hierarchical model is straightforward: I simply calculate $Q_i$ by averaging over all of the unknown parameters. In other words, I add extra uncertainty to the parties maximization problems by integrating the objective functions over $p(\beta, \delta_t, \sigma, \sigma_\delta)$ instead of just integrating over $h(\delta_t)$.

$Q_i$ is calculated in a similar manner when the model is estimated using the DLM. Because I separate the national vote and the district vote relative to the national vote, there is a probability distribution for each component. Due to the normality assumptions, both the forecasts of the national vote, $\mu_T$, and the district vote, $\xi_{iT}$ are normally distributed. $\sigma$ can consequently be estimated with the standard deviation of the posterior distribution of $\xi_T$. A posterior distribution for $V_i + \delta$ is simulated by summing the mean of the posterior distribution of $\xi_T$ and each simulated draw from the posterior distribution of $\mu_T$. In total, this provides a range of forecasts depending on the national error as well as an estimate of the district level error as required by the theory.

As discussed in~\autoref{sec: equilibrium}, party goals can have an important impact on $Q_i$. The implication of these goals are illustrated in~\autoref{fig: $Q_i$ versus the mean forecast of the Democratic vote in the 2008 election}, which shows how $Q_i$ depends on forecasted Democratic vote shares. When parties want to maximize the expected number of seats, they should spend the most resources on the closest elections. This is clear in the leftmost plot in which $Q_i$ follows a bell shaped pattern peaking when when the forecasted vote share is $0.5$. 

The strategies are more intricate when the parties maximize the probability of winning a majority of seats. In this case, the parties should spend the most resources on decisive swing districts, or districts that are most likely to be close when winning that district will cause one party to win one more seat than the other party. This, in turn, implies that the trailing party should spend more resources in districts that they are losing in an effort to make the election more unpredictable. These incentives are best illustrated in the 2008 election when the Democratic party was expected to win a large majority of seats.\footnote{Forecasts from the hierarchical model predict that the Republicans would have won, on average, $\predseats2008$ of the necessary 218 seats for a majority; in reality, they won 178 seats.} As shown in the plot, the Republican's optimal strategy (assuming they were only concerned with winning a majority of seats), was to to spend the most resources in districts with a predicted Democratic vote share over $0.5$. Since the number of seats won by both parties would have only have been close if the national swing shifted drastically in the Republican's favor, $Q_i$ is maximized for districts with a vote share close to $0.6$. Interestingly, the cost of spending on districts that Republicans were expected to barely win has such an adverse effect on the variance of the election that $Q_i$ is actually negative in these districts.
 
\begin{figure}[!htb]
\includegraphics{../figs/qplot.pdf}
\vspace{.5cm}
\caption{$Q_i$ versus the mean forecast of the Democratic vote in the 2008 election}
\label{fig: $Q_i$ versus the mean forecast of the Democratic vote in the 2008 election}
\begin{minipage}{\linewidth}
\footnotesize
\emph{Notes:} The forecasted Democratic vote share in each district is the mean of the posterior predictive distribution from the hierarchical model.
\end{minipage}
\end{figure}

\section{Relationship Between $\mathbf{Q_i}$ and District Spending}
This section examines the empirical relationship between the estimates of $Q_i$ and actual spending patterns by political parties, PACs and interest groups (see the online Appendix for an overview of campaign spending by group). The exact relationship between $Q_i$ and spending depends on the functional form of $u(e^J_i)$. It would be preferable to estimate a utility function and its parameters using empirical estimates of the effect of spending on votes, but results from the empirical literature are very imprecise. Instead, I use the logarithmic utility function analyzed in~\autoref{sec: functional form}, which yields an equilibrium in which $=e_i^{J*}/\sum e_i^{J*} =Q_i/\sum Q_i$. That is, I estimate the regression equation,
\begin{align} \label{eqn: regression eqn}
\frac{e_{iy}^{J}}{\sum e_{iy}^{J}} &= \gamma \frac{Q_{iy}}{\sum Q_{iy}} + X\beta + \eta,
\end{align} 
where $e_{iy}^{J}$ is observed spending in district $i$ and year $y$ and $X$ contains other covariates that affect district spending.
 
\subsection{Correlations}
\autoref{fig:Correlation Between Q and Spending, by Contributor Type} plots the correlation between $Q_i$ and spending by contributor type and the goal of the parties. The top and bottom panels show the correlation with spending between $Q_i^{maj}$ and $Q_i^{seats}$ respectively. The plot highlights two primary aspects of the data. First, there are few differences in spending patterns between contributor types. Surprisingly, the correlations for party affiliated groups are not significantly higher than the correlations for other groups. In addition, spending by individuals is actually the most correlated with the $Q_i$'s among all groups. 

Second, the correlations are not very sensitive to party goals except in 2008. In~\autoref{sec: calculating Q} I showed that assumptions about party goals impacted the values of $Q_i$ greatly in 2008 because the Democrats were expected to win the House by almost 80 seats. The 2008 election therefore provides a natural experiment that can be used to identify the goals that the parties actually have. The huge dip in the correlation between spending and $Q_i^{maj}$ but not between spending and $Q_i^{seats}$ suggests that parties maximize the expected number of seats rather than the probability of winning a majority of seats. 

\begin{figure}[!htb]
\includegraphics{../figs/corbytype.pdf}
\vspace{.5cm}
\caption{Correlation Between $\mathbf{Q_i}$ and Spending, by Contributor Type}
\label{fig:Correlation Between Q and Spending, by Contributor Type}
\begin{minipage}{\linewidth}
\footnotesize
\emph{Notes:} Spending is the sum of spending for Republicans and Democratics in a district by a given contributor type. The top panel shows the correlation between $Q_i^{maj}$ and spending while the bottom panel shows the correlation between $Q_i^{seats}$ and spending.
\end{minipage}
\end{figure}

Due to these results, I will now focus on total spending by PACs, party committees and individuals unless otherwise specified. According to Stromberg's model, if the effect of spending on vote share is of the log form, then both parties should spend the same proportion of their funds in each district. An examination of spending patterns by parties across districts is consistent with this notion. For instance, the simple correlation between spending on Democrats and spending on Republicans is $\corpartyexp$. Moreover, the online Appendix shows very little differences in the correlations between the $Q_i$'s and spending by party, although $Q_i^{seats}$ is slightly more correlated with spending on Republicans than spending on Democrats.


The correlations presented thus far use the hierarchical model to calculate $Q_i$. One would expect that the DLM could improve the fit between the model and the observed data. \autoref{fig:Correlation Between Q and PAC Spending by Date and Model, 2010 House Election} examines this by comparing the correlation between $Q_i^{seats}$ calculated using three different models and spending at different dates during the 2010 campaign.\footnote{The figure omits correlations made just prior to the election (late October) because the correlations are significantly lower and distort the figure: the correlations using the prior-informed DLM, no-prior DLM and hierarchical model are $\cordlmfinal$, $\cordlmnpfinal$ and $\corbhmfinal$ respectively. This likely occurs because there are only a couple of days between the final forecast date and election day.} Spending is the sum of all spending in a district between election day and the the date the forecast is made. The three models are the hierarchical model, the prior informed DLM and the non-prior informed DLM. 

In early September, or two months prior to the election, forecasts are only available from the hierarchical model so I assign all the models forecast values equal to the forecast from the hierarchical model. The figure shows that the $Q_i^{seats}$ calculated using the DLM's match actual spending better than the $Q_i^{seats}$ calculated using the hierarchical model, although the $Q_i^{seats}$ from the hierarchical model are still pretty accurate. The correlations between the $Q_i^{seats}$ estimated using the hierarchical model and spending decrease in a linear fashion over time while the correlations based on the DLM forecasts reach a peak of around 0.8 one month before the election.  There is little difference between the performance of the ``prior'' and ``no prior'' DLM's, although the correlations between the ``no prior'' DLM and spending are somewhat more variable. Overall, this figure suggests that campaign donors use polls to evaluate the competitiveness of a district and update these beliefs when new polls become available.

\begin{figure}[!htb]
\includegraphics{../figs/cormodels.pdf}
\vspace{.5cm}
\caption{Correlation Between $\mathbf{Q_i}$ and PAC Spending by Date and Model, 2010 House Election}
\label{fig:Correlation Between Q and PAC Spending by Date and Model, 2010 House Election}
\begin{minipage}{\linewidth}
\footnotesize
\emph{Notes:} Spending refers to all spending by PACs, party committees and individuals contributing over \$200 between a given date and election day.
\end{minipage}
\end{figure}

\subsection{Alternative Predictors of District Spending}
I now move beyond simple correlations to analyses using regression equation~\ref{eqn: regression eqn}. \autoref{table: OLS Regressions on Candidate Spending Shares} reports regression estimates of $\gamma$ and $\beta$ when analyzing spending on Democratic candidates (panel A) and Republican candidates (panel B) separately. Equilibrium spending, $Q_{iy}^{seats}/ \sum_i Q_{iy}^{seats}$, is estimated using the hierarchical model. Spending shares and equilibrium spending are multiplied by 100 so that they can be interpreted in percentage terms. Mean and median spending shares for Democratic and Republican candidates are $0.23\%$ and $0.085\%$ respectively for both parties.

The first column reports an estimate of a simple linear regression (with an intercept) with equilibrium spending as the only explanatory variable. The equilibrium spending coefficients in both panels A and B are strongly positively associated with actual spending and significantly different than $0$. The coefficients are somewhat inconsistent with the equilibrium spending conditions in equation~\ref{eqn: equilibrium spending} though because they are significantly different than $1$. That said, the coefficients are fairly close to $1$ and the $R^2$ values are high.

\begin{table}[!ht]
\footnotesize
\begin{center}
\begin{threeparttable}
\caption{OLS Regressions on Candidate Spending Shares (\%)} \label{table: OLS Regressions on Candidate Spending Shares}
\begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}d{-1}d{-1}d{-1}d{-1}d{-1}}
\vspace{-5pt}\\
\hline
\hline
\multicolumn{1}{c}{} & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)}& \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}& \multicolumn{1}{c}{(5)} \\
\hline
\multicolumn{1}{l}{\emph{Panel A. Democrats}} & \multicolumn{5}{c}{}\\
\ExpandableInput{../tables/regexp_dem.txt}
\multicolumn{1}{l}{District fixed effects?}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{Yes} \\
\hline
\multicolumn{1}{l}{\emph{Panel B. Republicans}} & \multicolumn{5}{c}{}\\
\ExpandableInput{../tables/regexp_rep.txt}
\multicolumn{1}{l}{District fixed effects?}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{Yes} \\
\hline
\hline
\end{tabular*}
\scriptsize
Notes: The dependent variable is the share of yearly spending by all PACs in each district. The unit of analysis is a candidate-district-year. Robust standard errors are in parentheses. \emph{Open seat}, \emph{Incumbent}, \emph{Ways and Means Committee}, \emph{Ways and Means Committee}, and \emph{Party Leadership} are indicator variables equal to 1 if candidates are running in open seats, incumbents, members of the Ways and Means Committee, chairs of a major committee or House party leaders. Party leaders include the speaker of the House, the majority/minority leaders and the majority/minority whip. \emph{Probability of victory} is the probability that a candidate wins the election calculated using the posterior predictive distribution from the hierarchical model.
\end{threeparttable}
\end{center}
\end{table}

The second column adds two dummy variables indicating that a candidate is an incumbent or running in an open seat (the omitted category is a challenger in an incumbent district). Each variable is statistically significant and positively associated with spending in both panels. The $R^2$ in column 2 improves considerably in the Democratic specification and marginally in the Republican one.

Columns 3 and 4 add influence-motivated variables that campaign donors might consider when deciding which campaigns to contribute to. The influence variables in column 3 are three indicator variables for whether candidates are member of the Ways and Means Committee, party leaders or committee chairmen. In column 4, I include an estimate of the probability that a candidate will win the election (0 to 1 scale) since campaign donors concerned with establishing a relationship with a candidate should prefer to donate to a candidate who will be in office \citep[see for instance][]{snyder1990campaign}. The influence related variables have a small impact on the $R^2$ and are often not statistically different than zero. Being a party leader is the most important influence variable and is positive and statistically significant in both panel A and panel B. 

To help alleviate concerns that the regression estimates might be biased, column 6 adds district fixed effect that control for time invariant district specific characteristics. The number of observations is smaller in this specification because the 2000 election is dropped since district lines changed in 2002. Adding the fixed effects improves the model fit and only has a small impact on the coefficients. Importantly, equilibrium spending remains a highly significant predictor of actual spending.

As a whole, the regression estimates reported in~\autoref{table:Spending by the Financial Industry, Incumbent Races 2000 - 2010} show that equilibrium spending explains a large amount of the variation in actual spending and that the relationship between equilibrium and actual spending is robust to district fixed effects. A few other variables such as whether the candidate is an incumbent or the seat is open help explain the data a little bit better (especially for Democrats) but little is gained in terms of fit from adding influence related variables to the regression.

While spending in a district may be largely explained by equilibrium spending, this ignores some important heterogeneity across campaign donors. Organizations whose welfare depends heavily on policy decisions should have the largest incentives to influence policy. \autoref{table:Spending by the Financial Industry, Incumbent Races 2000 - 2010} looks at whether this is the case by analyzing the spending decisions of the financial industry, a group whose campaign goals might differ markedly from other organizations. Campaign contributions from the financial industry are defined as those coming from PACs classified as a member of the Finance, Insurance, and Real Estate industry by the CRP or from individuals employed in the same industry. The regression analysis examines the impact that being a member of the House Committee on Financial Services Committee has on contributions from these firms. The unit of analysis for the regressions is an incumbent candidate since challengers cannot serve on a congressional committee. The dependent variable is the district share of yearly spending by the financial industry on incumbents. 

\begin{table}[!ht]
\footnotesize
\begin{center}
\begin{threeparttable}
\caption{Spending by the Financial Industry, Incumbent Races 2000 - 2010} \label{table:Spending by the Financial Industry, Incumbent Races 2000 - 2010}
\begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}d{-1}d{-1}d{-1}}
\vspace{-5pt}\\
\hline
\hline
\multicolumn{1}{c}{Variable} & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)}& \multicolumn{1}{c}{(3)} \\
\hline
\ExpandableInput{../tables/regexp_fin.txt}
\multicolumn{1}{l}{District fixed effects?}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{No}& \multicolumn{1}{c}{Yes} \\
\hline
\hline
\end{tabular*}
\scriptsize
Notes: The dependent variable is the district share of yearly spending by the Finance, Insurance, and Real Estate industry on incumbents. Industry spending covers contributions from PACs and individuals (sorted by employer) donating at least \$200. The unit of analysis a district-year. Open seat districts are excluded from the analysis. Robust standard errors are in parentheses. 
\end{threeparttable}
\end{center}
\end{table}

Column 1 shows that equilibrium spending is positive and statistically significant. Column 2 includes an indicator variables equal to 1 if an incumbent is a member of the Financial Services Committee and 0 otherwise. Since the chair and ranking minority members of the Financial Services Committee are likely to hold the most influence, I also include a dummy variable indicating such status. Both variables are statistically significant and have large positive coefficients. In column 3, which controls for district fixed effects, equilibrium spending and committee status remain statistically different than zero but being a chair or ranking minority member does not. This should not be seen as an evidence that a leadership position on the committee is unimportant, but rather a consequence of a fixed effect regression without enough variation in an explanatory variable. In summary, the financial industry appears to pursue both election motivated and influence motivated spending strategies.

\section{Discussion and Summary}
This paper uses Stromberg's \citeyear{stromberg2008electoral} probabilistic voting model to quantify the amount that political parties should spend on districts in House elections. The calculations are made using two assumptions about goals: first, parties maximize the expected number of seats and second, parties maximize the probability of winning a majority of seats. The model is estimated using a Bayesian forecasting model that is capable of updating forecasts in real-time as new polls become available.

The assumptions regarding the parties' goals have important implications for the optimal allocation of resources. Under the first assumption, parties should spend the most on close districts. On the other hand, when the second assumption is true, parties should spend the most on decisive swing districts; that is, districts that are close when they are pivotal in whether a party wins or loses a majority of seats. In both cases, spending should be more concentrated when the forecasts are more precise. 

The empirical results support the first assumption but not the second one. For instance, the correlation between district spending and the amount that should have been spent if parties were maximizing the expected number of seats ranges from 0.5 to 0.8 depending on the date, party of the candidate and forecasting method. Conversely, during the 2008 election in which optimal strategies differed greatly because the Democrats were predicted to win a huge majority, observed spending is highly correlated with a spending strategy based on maximizing the expected number of seats but not with with a spending strategy based on maximizing the probability of winning a majority of seats. That said, one cannot rule out the possibility that the parties maximize total seats in lopsided elections in order to increase the probability of majority control in the future. 

The empirical results also suggest that most political giving is done to affect elections rather than for other reasons like protecting incumbents or influencing candidates. That said, some political actors, like the financial industry, place close to equal weight on influencing elections and gaining access to politicians.

One curious finding that warrants discussion is that the correlation between equilibrium spending under the model and actual spending is nearly identical for political parties, individuals, and PACs. There are two competing explanations for this. First, it could suggest that the national parties are able to coordinate campaign efforts with individuals and groups that have similar political goals. This explanation is consistent with Herrnson's \citeyear{herrnson2009roles} view of political parties as multilayered coalitions.

However, a second explanation that cannot be ruled out is that candidates have larger incentives to raise funds in tight races. This interpretation is consistent with the game theoretic model in \citet{erikson2000equilibria}. Intuitively, this occurs because an additional dollar of spending has a larger impact on the probability that a candidate will win an election when the projected vote share is closest to 1/2. There is some empirical support for this as well. For example, \citet{stein1994congressional} show that the most vulnerable candidates for reelection (as measured by their vote share in the previous election) are the most likely to obtain new grant money for their constituents. Similarly, \citet{bickers1996electoral} find that vulnerable incumbents use the grant system to deter quality challengers from opposing them.

There are a couple of fruitful areas for future research. First, researchers should try to separate the competing theories of why close elections receive more campaign contributions. Second, it would be beneficial to separate campaign spending into distinct categories. An analysis of TV and radio advertisements might be particularly interesting, although it would require more detailed information about advertising prices. Nonetheless, such an analysis would be particularly useful for deriving strategies applicable to real-world campaigns.
 
\pdfbookmark[1]{References}{References}
\bibliography{bibliography}

\end{document} 
